{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd05e201434fc87e731f1f08451237b3d562d890a8afaf10be68eaaa2bf1fd600d6",
   "display_name": "Python 3.9.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Task 6 - Twitter Sentiment Classification\n",
    "\n",
    "*by Lukas DÃ¶tlinger*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "columns = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
    "data = pd.read_csv('res/training.1600000.processed.noemoticon.csv', names=columns)\n",
    "\n",
    "texts = data['text'].tolist()\n",
    "\n",
    "negative_tweets = data[data['target'] == 0]['text'].tolist()\n",
    "positive_tweets = data[data['target'] == 4]['text'].tolist()\n"
   ]
  },
  {
   "source": [
    "We can split the tweets in the dataset into two categories based on their predefined classification. We can then use existing specifiers like the `SentimentIntensityAnalyzer` from `nltk` to compute a score for each text. To get some base values we use the `compute_results()` function below, which utilizes the compount polarity score. That score indicates that a text is positive for values greater 0 and negative for others."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Precision: 0.7560344827586207\nRecall: 0.877\nF1: 0.812037037037037\nAccuracy: 0.797\n"
     ]
    }
   ],
   "source": [
    "def compute_results(n_tweets, p_tweets):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    n_count = len(n_tweets)\n",
    "    n_errors = 0\n",
    "    for n_t in n_tweets:\n",
    "        if sia.polarity_scores(n_t)['compound'] > 0:\n",
    "            n_errors += 1\n",
    "\n",
    "    p_count = len(p_tweets)\n",
    "    p_errors = 0\n",
    "    for p_t in p_tweets:\n",
    "        if sia.polarity_scores(p_t)['compound'] < 0:\n",
    "            p_errors += 1\n",
    "\n",
    "    tp = p_count - p_errors\n",
    "    fn = p_errors\n",
    "    fp = n_errors\n",
    "    tn = n_count - n_errors\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1: {(2 * precision * recall) / (precision + recall)}')\n",
    "    print(f'Accuracy: {(tp + tn) / (tp + fp + tn + fn)}')\n",
    "\n",
    "compute_results(negative_tweets, positive_tweets)"
   ]
  },
  {
   "source": [
    "The results show that the sentiment classifier is already very capable with an acuracy of about 79 percent. We can further try to optimize this by only using alphanumeric lowercase words or removing stopwords. In this case I tried tokenizing with lowercase words and removal of non-alphanumeric characters, removing stopwords, using lemmatization and combining all approaches."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Precision: 0.7469982847341338\nRecall: 0.871\nF1: 0.804247460757156\nAccuracy: 0.788\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def filter_tweets(t):\n",
    "    return ' '.join([ w.lower() for w in word_tokenize(t) if w.isalpha() ])\n",
    "\n",
    "def lowercase_alpha(tweets):\n",
    "    return list(map(filter_tweets, tweets))\n",
    "    \n",
    "la_negative = lowercase_alpha(negative_tweets)\n",
    "la_positive = lowercase_alpha(positive_tweets)\n",
    "compute_results(la_negative, la_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Precision: 0.741306191687871\nRecall: 0.874\nF1: 0.8022028453419\nAccuracy: 0.7845\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(t):\n",
    "    return ' '.join([ w for w in word_tokenize(t) if not w in stop_words ])\n",
    "\n",
    "def no_stopwords(tweets):\n",
    "    return list(map(remove_stopwords, tweets))\n",
    "\n",
    "nos_negative = no_stopwords(negative_tweets)\n",
    "nos_positive = no_stopwords(positive_tweets)\n",
    "compute_results(nos_negative, nos_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Precision: 0.7425658453695837\nRecall: 0.874\nF1: 0.802939825447864\nAccuracy: 0.7855\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tweet(t):\n",
    "    return ' '.join([ lemmatizer.lemmatize(w) for w in word_tokenize(t) ])\n",
    "\n",
    "def lemmatized(tweets):\n",
    "    return list(map(lemmatize_tweet, tweets))\n",
    "\n",
    "lem_negative = lemmatized(negative_tweets)\n",
    "lem_positive = lemmatized(positive_tweets)\n",
    "compute_results(lem_negative, lem_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Precision: 0.7208192573056208\nRecall: 0.901225\nF1: 0.8009896551704984\nAccuracy: 0.776085625\n"
     ]
    }
   ],
   "source": [
    "def clean(t):\n",
    "    return ' '.join([ lemmatizer.lemmatize(w.lower()) for w in word_tokenize(t) if not w in stop_words ])\n",
    "\n",
    "def combined(tweets):\n",
    "    return list(map(clean, tweets))\n",
    "\n",
    "clean_negative = combined(negative_tweets)\n",
    "clean_positive = combined(positive_tweets)\n",
    "compute_results(clean_negative, clean_positive)"
   ]
  },
  {
   "source": [
    "As we can see from the results, nothing really seems to improve the classification. We can conclude that the classifier is already well trained for the tweets in the test set. This is somewhat expected since it was trained on social media texts. When combining all approaches we even get the worst result of all. However, precision is only lowered for about three percent, which shows that the changes don't really have a high impact."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}