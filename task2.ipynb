{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "missing-agent",
   "metadata": {},
   "source": [
    "# Task 2: Stemming and Lemmatizing\n",
    "\n",
    "*by Lukas Dötlinger*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "isolated-prairie",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer:\n",
      "['sentence -> sentenc',\n",
      " 'was -> wa',\n",
      " 'used -> use',\n",
      " 'doing -> do',\n",
      " 'stemming -> stem',\n",
      " 'using -> use',\n",
      " 'multiple -> multipl',\n",
      " 'stemmers -> stemmer',\n",
      " 'implemented -> implement',\n",
      " 'This -> thi',\n",
      " 'groundbreaking -> groundbreak',\n",
      " 'results -> result',\n",
      " 'conclude -> conclud',\n",
      " 'results -> result',\n",
      " 'different -> differ',\n",
      " 'overall -> overal',\n",
      " 'using -> use',\n",
      " 'different -> differ',\n",
      " 'stemmers -> stemmer',\n",
      " 'During -> dure',\n",
      " 'testing -> test',\n",
      " 'only -> onli',\n",
      " 'differences -> differ',\n",
      " 'approaches -> approach']\n",
      "\n",
      "Snowball Stemmer:\n",
      "['A -> a',\n",
      " 'sentence -> sentenc',\n",
      " 'used -> use',\n",
      " 'doing -> do',\n",
      " 'stemming -> stem',\n",
      " 'using -> use',\n",
      " 'multiple -> multipl',\n",
      " 'stemmers -> stemmer',\n",
      " 'implemented -> implement',\n",
      " 'This -> this',\n",
      " 'groundbreaking -> groundbreak',\n",
      " 'results -> result',\n",
      " 'We -> we',\n",
      " 'conclude -> conclud',\n",
      " 'results -> result',\n",
      " 'different -> differ',\n",
      " 'overall -> overal',\n",
      " 'using -> use',\n",
      " 'different -> differ',\n",
      " 'stemmers -> stemmer',\n",
      " 'During -> dure',\n",
      " 'testing -> test',\n",
      " 'only -> onli',\n",
      " 'differences -> differ',\n",
      " 'approaches -> approach']\n",
      "\n",
      "Intersection when using all words:\n",
      "['This -> this', 'was -> wa', 'This -> thi', 'We -> we', 'A -> a']\n",
      "\n",
      "Intersection when excluding stopwords:\n",
      "['This -> this', 'This -> thi', 'We -> we', 'A -> a']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "text = \"\"\"A longer sentence was used for doing some stemming by using multiple stemmers which\n",
    "    are implemented in nltk. This didn't yield an groundbreaking results. We can conclude\n",
    "    that the results are not that different overall when using different stemmers. During the\n",
    "    testing, only some minor differences were found between the approaches.\"\"\"\n",
    "words = word_tokenize(text)\n",
    "filtered_words = [w for w in words if not w in stop_words]\n",
    "\n",
    "ps = PorterStemmer()\n",
    "ss = SnowballStemmer('english')\n",
    "\n",
    "# Stems words and returns the transformation as string if stemmer changes word.\n",
    "stem_words = lambda words, s: ['{} -> {}'.format(w, s.stem(w)) for w in words if w != s.stem(w)]\n",
    "\n",
    "ps_result = stem_words(words, ps)\n",
    "ss_result = stem_words(words, ss)\n",
    "print('Porter Stemmer:')\n",
    "pprint(ps_result)\n",
    "print('\\nSnowball Stemmer:')\n",
    "pprint(ss_result)\n",
    "\n",
    "print('\\nIntersection when using all words:')\n",
    "pprint(list(set(ss_result) ^ set(ps_result)))\n",
    "\n",
    "ps_result = stem_words(filtered_words, ps)\n",
    "ss_result = stem_words(filtered_words, ss)\n",
    "\n",
    "print('\\nIntersection when excluding stopwords:')\n",
    "pprint(list(set(ss_result) ^ set(ps_result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-roller",
   "metadata": {},
   "source": [
    "When looking at the computed differences, we cannot observe any major difference between the two stemmers. However the `SnowballStemmer` seems to be a bit more accurate and less aggressive, as the `PorterStemmer` stems `this` to `thi` and `was` to `wa`. We can therefore argue that a *Snowball Stemmer* is better for the given sentences. The removal of stopwords does not change that in any way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "understood-arthritis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized words that changed:\n",
      "was -> wa\n",
      "stemmers -> stemmer\n",
      "results -> result\n",
      "results -> result\n",
      "stemmers -> stemmer\n",
      "differences -> difference\n",
      "approaches -> approach\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"\"\"A longer sentence was used for doing some stemming by using multiple stemmers which\n",
    "    are implemented in nltk. This didn't yield an groundbreaking results. We can conclude\n",
    "    that the results are not that different overall when using different stemmers. During the\n",
    "    testing, only some minor differences were found between the approaches.\"\"\"\n",
    "words = word_tokenize(text)\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print('Lemmatized words that changed:')\n",
    "for w in words:\n",
    "    if (wordnet_lemmatizer.lemmatize(w) != w):\n",
    "        print('{} -> {}'.format(w, wordnet_lemmatizer.lemmatize(w)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-awareness",
   "metadata": {},
   "source": [
    "Comparing the `WordNetLemmatizer` to the previously used stemmers shows that it's only partially different when looking at the intersection of their results. Overall we can observe that the lemmatizer transforms way less words in total. This is due to the lemmatizer beeing a more intelligent approach that laverages a dictionary to find the proper form of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "proved-brown",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lange -> lang',\n",
      " 'zusammenhangslos -> zusammenhangslo',\n",
      " 'oftmals -> oftmal',\n",
      " 'schlechterer -> schlechter',\n",
      " 'als -> al',\n",
      " 'andere -> ander',\n",
      " 'werde -> werd',\n",
      " 'andere -> ander',\n",
      " 'Texte -> text',\n",
      " 'besonders -> besond',\n",
      " 'diese -> dies',\n",
      " 'als -> al',\n",
      " 'etwas -> etwa',\n",
      " 'Texte -> text']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# https://designers-inn.de/blindtexte/\n",
    "text = \"\"\"Hallo. Ich bin ein kleiner Blindtext. Und zwar schon so lange ich denken kann.\n",
    "    Es war nicht leicht zu verstehen, was es bedeutet, ein blinder Text zu sein: Man\n",
    "    ergibt keinen Sinn. Wirklich keinen Sinn. Man wird zusammenhangslos eingeschoben\n",
    "    und rumgedreht – und oftmals gar nicht erst gelesen. Aber bin ich allein deshalb ein\n",
    "    schlechterer Text als andere? Na gut, ich werde nie in den Bestsellerlisten stehen.\n",
    "    Aber andere Texte schaffen das auch nicht. Und darum stört es mich nicht besonders\n",
    "    blind zu sein. Und sollten Sie diese Zeilen noch immer lesen, so habe ich als\n",
    "    kleiner Blindtext etwas geschafft, wovon all die richtigen und wichtigen Texte meist\n",
    "    nur träumen.\"\"\"\n",
    "words = word_tokenize(text)\n",
    "ss = SnowballStemmer('english')\n",
    "\n",
    "# Outputs words which are different after stemming (not case sensitive)\n",
    "pprint(['{} -> {}'.format(w, ss.stem(w)) for w in words if w.lower() != ss.stem(w)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
