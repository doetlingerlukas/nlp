{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "5e201434fc87e731f1f08451237b3d562d890a8afaf10be68eaaa2bf1fd600d6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Task 3\n",
    "\n",
    "*by Lukas DÃ¶tlinger*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "nps_chat top 100 - gutenberg top 100\n('part', 1022) - ('i', 30221)\n('join', 1021) - ('and', 16596)\n('lol', 822) - ('shall', 11682)\n('hi', 656) - ('said', 9429)\n('i', 576) - ('unto', 9010)\n('action', 347) - ('lord', 8590)\n('hey', 292) - ('the', 7835)\n('u', 204) - ('thou', 6759)\n('like', 160) - ('one', 6218)\n('im', 149) - ('man', 5615)\n('pm', 149) - ('thy', 5609)\n('chat', 146) - ('god', 5287)\n('good', 132) - ('thee', 4807)\n('lmao', 128) - ('but', 4692)\n('wanna', 110) - ('ye', 4674)\n('ok', 106) - ('upon', 4587)\n('know', 104) - ('would', 4046)\n('get', 104) - ('he', 3659)\n('room', 103) - ('come', 3642)\n('ya', 100) - ('could', 3594)\n('wb', 96) - ('like', 3468)\n('oh', 96) - ('came', 3337)\n('hello', 92) - ('day', 3326)\n('well', 91) - ('king', 3149)\n('one', 91) - ('little', 3065)\n('yes', 87) - ('know', 3057)\n('yeah', 85) - ('house', 2900)\n('hiya', 85) - ('good', 2899)\n('back', 79) - ('every', 2892)\n('see', 78) - ('see', 2885)\n('go', 77) - ('great', 2859)\n('dont', 77) - ('go', 2841)\n('ty', 72) - ('us', 2804)\n('want', 71) - ('people', 2802)\n('got', 68) - ('son', 2754)\n('everyone', 67) - ('made', 2744)\n('love', 63) - ('say', 2737)\n('anyone', 62) - ('men', 2726)\n('guys', 59) - ('time', 2707)\n('talk', 58) - ('let', 2676)\n('thanks', 57) - ('for', 2667)\n('would', 55) - ('israel', 2592)\n('right', 55) - ('it', 2569)\n('think', 55) - ('may', 2549)\n('nice', 54) - ('hath', 2535)\n('time', 52) - ('well', 2509)\n('you', 51) - ('even', 2480)\n('f', 50) - ('hand', 2446)\n('girls', 50) - ('two', 2374)\n('haha', 46) - ('mr', 2356)\n('bye', 46) - ('must', 2333)\n('thats', 46) - ('father', 2319)\n('welcome', 46) - ('much', 2308)\n('girl', 45) - ('went', 2307)\n('never', 45) - ('make', 2255)\n('make', 44) - ('children', 2253)\n('need', 44) - ('yet', 2234)\n('whats', 43) - ('also', 2186)\n('really', 43) - ('then', 2184)\n('night', 43) - ('old', 2081)\n('people', 43) - ('land', 2081)\n('song', 42) - ('away', 2027)\n('mode', 42) - ('o', 1994)\n('much', 40) - ('things', 1977)\n('man', 39) - ('thing', 1974)\n('take', 38) - ('might', 1963)\n('hot', 38) - ('first', 1937)\n('work', 38) - ('way', 1917)\n('even', 37) - ('mrs', 1857)\n('gonna', 37) - ('never', 1838)\n('omg', 36) - ('give', 1830)\n('say', 36) - ('take', 1803)\n('damn', 36) - ('long', 1765)\n('seen', 36) - ('you', 1750)\n('come', 36) - ('saying', 1746)\n('still', 34) - ('though', 1738)\n('nick', 33) - ('think', 1693)\n('brb', 33) - ('earth', 1682)\n('day', 33) - ('shalt', 1673)\n('gay', 31) - ('what', 1659)\n('sorry', 31) - ('ever', 1656)\n('something', 31) - ('she', 1644)\n('r', 30) - ('saw', 1636)\n('long', 30) - ('therefore', 1628)\n('hell', 29) - ('in', 1621)\n('wow', 29) - ('to', 1614)\n('someone', 29) - ('heard', 1599)\n('last', 29) - ('put', 1592)\n('today', 29) - ('many', 1591)\n('tell', 29) - ('place', 1588)\n('name', 28) - ('without', 1581)\n('didnt', 28) - ('they', 1568)\n('away', 28) - ('that', 1509)\n('ladies', 28) - ('heart', 1500)\n('sure', 28) - ('head', 1495)\n('going', 28) - ('name', 1493)\n('pic', 27) - ('nothing', 1469)\n('look', 27) - ('behold', 1468)\n('please', 26) - ('a', 1456)\n('ca', 26) - ('so', 1453)\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import nps_chat, gutenberg, stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filter_words = lambda words, filter_words: [ word.lower() for word in words if word.isalpha() and word not in filter_words ]\n",
    "\n",
    "nps_chat_dist = FreqDist(filter_words(nps_chat.words(), stop_words))\n",
    "gutenberg_dist = FreqDist(filter_words(gutenberg.words(), stop_words))\n",
    "\n",
    "nps_most_common = nps_chat_dist.most_common(100)\n",
    "gutenberg_most_common = gutenberg_dist.most_common(100)\n",
    "\n",
    "print('nps_chat top 100 - gutenberg top 100')\n",
    "for n, g in zip(nps_most_common, gutenberg_most_common):\n",
    "    print('{} - {}'.format(n, g))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}